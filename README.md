# ðŸ“ˆ Numerical Optimization  

This repository contains implementations of various **gradient descent optimization techniques** from scratch and using libraries. These methods are fundamental in machine learning and deep learning optimization.  

## ðŸš€ Topics Covered  

### ðŸ”¹ Gradient Descent Variants  
- **Vanilla Gradient Descent** (from scratch & using libraries)  
- **Batch, Mini-Batch, and Stochastic Gradient Descent (SGD)**  
- **Momentum-based Gradient Descent**  
- **Adaptive Gradient Descent Algorithms (Adagrad, RMSprop, Adam, etc.)**  

